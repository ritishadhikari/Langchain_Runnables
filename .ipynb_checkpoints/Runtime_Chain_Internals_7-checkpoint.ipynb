{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "591c6f01-7ea4-473b-be98-9ab8aa73dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough,RunnableLambda, Runnable, RunnableParallel,RunnableConfig,RunnableGenerator\n",
    "from langchain_core.messages import AIMessage\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import tool\n",
    "from langchain.prompts import ChatPromptTemplate,SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from operator import itemgetter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "import json\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import FAISS,Chroma\n",
    "from operator import itemgetter\n",
    "import time\n",
    "import grandalf\n",
    "from typing import Iterator,List,AsyncIterator\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain.runnables.hub import HubRunnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dc55a1a-cb69-4166-8b51-eab998d3b40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv(\"D:\\LLM Courses\\Master Langchain Udemy\\.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4fcdc85-d669-4029-bba4-f0130ec5bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f43a982-2e3a-4a28-b85c-948f41da974d",
   "metadata": {},
   "source": [
    "<h3> Configurable Fields </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6f2e9d44-4319-4c01-adec-9127373f67e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChatOpenAI(temperature=0)\\\n",
    "    .configurable_fields\\\n",
    "        (\n",
    "            temperature=ConfigurableField\n",
    "                            (\n",
    "                                id=\"llmTemperature\",\n",
    "                                name=\"LLM Temperature\",\n",
    "                                description=\"The temperature of the LLM\"\n",
    "                            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c3d6922d-010c-4160-9425-2fcd990dced4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='7', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 17, 'total_tokens': 18, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e27b3c90-2151-4bd7-9d3c-bc40da4b275c-0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(input=\"Select a Random Number between 1 and 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b68060fe-ad00-4e6d-b5d9-5f2a564a2a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StrOutputParser().invoke(model.invoke(input=\"Select a Random Number\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8573f372-04e8-44e3-90a5-123a89d3d55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='17', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 11, 'total_tokens': 12, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-310020c4-1b76-4a80-9209-b068f96da290-0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.with_config(configurable={\"llmTemperature\":0.3}).invoke(input=\"Select a Random Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "36a338cb-9aa1-412c-b64e-35ea55cfee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_template(template=\"Pick a random Number above {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f030361e-a343-4c12-813f-f95e2369f65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt|llm|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b429653e-1b9b-4ad9-b75c-2df0df858da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I can't pick a truly random number, as I'm a language model, not a random number generator.  \\n\\nHowever, I can give you a number above 23: **24**. \\n\\nIf you need a truly random number, you can use a website or a tool that generates random numbers. \\n\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.with_config(configurable={\"llmTemperature\":0.2}).invoke(input={\"x\":23})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252f95e-cd31-4248-9fb4-e4fbe93e9021",
   "metadata": {},
   "source": [
    "<h3>Hub Runnable</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "92ef15b4-580a-4972-a291-fb2820ec4edf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt=HubRunnable(owner_repo_commit=\"rlm/rag-prompt\").configurable_fields(\n",
    "    owner_repo_commit=ConfigurableField(\n",
    "        id=\"hub_commit\",\n",
    "        name=\"Hub Commit\",\n",
    "        description=\"The Hub Commit to Pull from\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "120b70a4-09a1-4ba8-8979-fdd521a5cf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: {question} \n",
      "Context: {context} \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompt.default.bound.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7c4daa0a-0fbc-41bc-9ca0-ac846c664a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: foo \n",
      "Context: bar \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompt.invoke(input={\"question\":\"foo\",\"context\":\"bar\"}).messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bcb97424-5383-48c4-a7a9-9d9924385c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>> \n",
      "Question: foo \n",
      "Context: bar \n",
      "Answer: [/INST]\n"
     ]
    }
   ],
   "source": [
    "print(prompt.with_config(\n",
    "    configurable={\"hub_commit\":\"rlm/rag-prompt-llama\"}\n",
    ").invoke(input={\"question\":\"foo\",\"context\":\"bar\"}).messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381c4825-98a9-410d-ade4-a9233b93ab5a",
   "metadata": {},
   "source": [
    "<h3> Configurable Alternatives</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "315836cd-a347-4d77-a6db-6710c09307ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGoogleGenerativeAI(temperature=0,model=\"gemini-1.5-flash\").configurable_alternatives(\n",
    "    which=ConfigurableField(id=\"llm\"),\n",
    "    default_key=\"gemini\",\n",
    "    openai=ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0)\n",
    "    ).configurable_fields(temperature=ConfigurableField(\n",
    "            id=\"llmTemperature\",\n",
    "            name=\"LLM Temperature\",\n",
    "            description=\"The temperature of the LLM\"  \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3a39b372-0da8-431b-9c10-02390f491ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_template(\n",
    "    template=\"Tell me a joke about {topic}\"\n",
    ")\n",
    "\n",
    "chain=prompt|llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7807f1c8-085f-41b5-976b-00ea39249c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the giraffe get a job at the library? \\n\\nBecause he was really good at reaching for the high shelves! ðŸ¦’ðŸ“š \\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-be08cc8d-cbef-4765-ac01-26399cf5d72d-0')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={\"topic\":\"Giraffe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "449ed7c6-a2cb-4ff0-9a56-af1290dbd6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the giraffe break up with the zebra? Because he was always telling tall tales!', response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 14, 'total_tokens': 34, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-720280ec-affe-4e23-8e3a-79ab6601289b-0')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.with_config(\n",
    "    configurable={\"llm\":\"openai\",\"llmTemperature\":0.6}\n",
    ").invoke(input={\"topic\":\"Giraffe\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a943dc7-85ef-40be-a51c-2097efcc26c9",
   "metadata": {},
   "source": [
    "<h3>With Prompts</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "747b06f8-ecdc-455e-8f76-1766b072034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_template(\n",
    "    template=\"Tell me a joke about {topic}\"\n",
    ").configurable_alternatives(\n",
    "    ConfigurableField(id=\"prompt\"),\n",
    "    default_key=\"joke\",\n",
    "    poem=ChatPromptTemplate.from_template(\n",
    "            template=\"Tell me a poem about {topic}\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9c0cc32e-85e4-4208-bb7e-bb3371a1d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt|llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b58de0b1-b270-4234-baa7-5861d40f53d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='In the face of trials and tribulations,\\nPersistence is our guiding light,\\nThrough the darkest of situations,\\nIt keeps us pushing with all our might.\\n\\nWhen obstacles block our way,\\nAnd failure seems imminent,\\nPersistence helps us stay,\\nFocused and resilient.\\n\\nIt\\'s the fuel that drives us forward,\\nWhen all hope seems lost,\\nIt\\'s the voice that whispers, \"You can do it\",\\nNo matter the cost.\\n\\nWith persistence as our ally,\\nWe can conquer any feat,\\nFor with unwavering determination,\\nSuccess is always sweet.\\n\\nSo let us never give up,\\nNo matter how tough the road,\\nFor with persistence as our compass,\\nWe will reach our ultimate goal.', response_metadata={'token_usage': {'completion_tokens': 135, 'prompt_tokens': 13, 'total_tokens': 148, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8b26f337-722f-41db-9994-419f658b34d7-0')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.with_config(\n",
    "    configurable={\"llm\":\"openai\",\"llmTemperature\":0.6,'prompt':'poem'}\n",
    ").invoke(input={\"topic\":\"Persistence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b455add0-9ad4-4fe7-ace4-4c0d3d6ff200",
   "metadata": {},
   "outputs": [],
   "source": [
    "openAIwithPoemPrompt=chain.with_config(\n",
    "    configurable={\"llm\":\"openai\",\"llmTemperature\":0.6,'prompt':'poem'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "08b5054a-f590-4704-a6e0-83a8da371dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the stillness of the mind,\n",
      "Where thoughts and worries unwind,\n",
      "There lies a place of peace,\n",
      "Where all troubles cease.\n",
      "\n",
      "In the garden of Zen,\n",
      "Where the lotus blooms again,\n",
      "The mind is clear and free,\n",
      "Like a calm and tranquil sea.\n",
      "\n",
      "Breathing in, breathing out,\n",
      "Letting go of fear and doubt,\n",
      "Finding harmony within,\n",
      "In the practice of Zen.\n",
      "\n",
      "No need for words or explanations,\n",
      "Just be present in each sensation,\n",
      "In the here and now we find,\n",
      "The true nature of the mind.\n",
      "\n",
      "So let go of all attachment,\n",
      "And find inner contentment,\n",
      "In the simplicity of Zen,\n",
      "Where enlightenment begins.\n"
     ]
    }
   ],
   "source": [
    "print(openAIwithPoemPrompt.invoke(input={\"topic\":\"Zen\"}).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414863ef-fb5d-4637-a8e7-70a96f32d021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
