{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18589721-1eb3-46a8-b213-bbe19228adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough,RunnableLambda, Runnable, RunnableParallel,RunnableConfig,RunnableGenerator\n",
    "from langchain_core.messages import AIMessage\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import tool\n",
    "from langchain.prompts import ChatPromptTemplate,SystemMessagePromptTemplate, HumanMessagePromptTemplate,PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser,StrOutputParser\n",
    "from operator import itemgetter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "import json\n",
    "from langchain_community.vectorstores import FAISS,Chroma\n",
    "from operator import itemgetter\n",
    "import time\n",
    "import grandalf\n",
    "from typing import Iterator,List,AsyncIterator\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain.runnables.hub import HubRunnable\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.utils.math import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7194882-b1da-4dfe-8f41-289e5397a408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv(\"D:\\LLM Courses\\Master Langchain Udemy\\.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f16a68f3-ae53-4522-923b-4d560d803aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmGemini=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "llmGPT=ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28e1c8-47fc-422f-bb87-c0cd8440feac",
   "metadata": {},
   "source": [
    "<h3>Using Runnable Lambda</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3619582-e740-4886-b579-e7f3d94e1466",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate.from_template(template=\"\"\"\n",
    "    Given the user question below, classify it as either being about `Langchain`,`Anthropic`, or `Other`\n",
    "\n",
    "    Do not respond with more than one word.\n",
    "\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "\n",
    "    Classification:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88e471f9-2938-4457-bc74-106289e6b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt|llmGemini|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e564fce7-06e4-486b-a61d-f6c3c145b627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anthropic \\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={\"question\":\"What is Claude LLM?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c6ff89a-a21d-474c-968a-fc66877dcf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchainChain=PromptTemplate.from_template(\n",
    "    template=\"\"\"\n",
    "        You are an expert in langchain.\\\n",
    "        Always answer questions starting with \"As Prof. Langchain told me\". \\\n",
    "        Respond to the following question:\n",
    "\n",
    "        Question: {question}\n",
    "        Answer:\n",
    "    \"\"\"\n",
    ")| llmGemini|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c13477e8-87f5-4623-beae-193aecfa2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropicChain=PromptTemplate.from_template(\n",
    "    template=\"\"\"\n",
    "        You are an expert in anthropic.\\\n",
    "        Always answer questions starting with \"As Prof. Anthropic told me\". \\\n",
    "        Respond to the following question:\n",
    "\n",
    "        Question: {question}\n",
    "        Answer:\n",
    "    \"\"\"\n",
    ")| llmGemini|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "383a61fd-dcb6-4366-9624-6cfed57c496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generalChain=PromptTemplate.from_template(\n",
    "    template=\"\"\"\n",
    "        Respond to the following question:\n",
    "\n",
    "        Question: {question}\n",
    "        Answer:\n",
    "    \"\"\"\n",
    ")| llmGemini|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fcd5110-f532-4932-8907-ff13ec5defa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route(info):\n",
    "    if \"anthropic\" in info[\"topic\"].lower():\n",
    "        return anthropicChain\n",
    "    elif \"langchain\" in info[\"topic\"].lower():\n",
    "        return langchainChain\n",
    "    else:\n",
    "        return generalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84267ce3-ccf1-46c2-b3dc-0b02d3c1a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalChain=RunnableParallel(\n",
    "    {\"topic\":chain,\"question\":RunnablePassthrough()}\n",
    ")|RunnableLambda(func=route)|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69b491d8-5c46-4959-b1ce-b3be98c96c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As Prof. Anthropic told me, Claude is a large language model developed by Anthropic. It is a powerful AI system designed to generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way. Claude is still under development, but it has already learned to perform many kinds of tasks, including:\\n\\n* **Summarizing text:** Claude can condense long pieces of writing into shorter summaries, highlighting the key points.\\n* **Generating different creative text formats:** Claude can write poems, code, scripts, musical pieces, email, letters, etc. It will try its best to fulfill all your requirements.\\n* **Answering your questions in an informative way:** Claude is trained on a massive dataset of text and code, and it can access and process information from the real world through Google Search. This allows it to provide comprehensive and informative answers to your questions.\\n\\nClaude is still under development, but it has already learned to perform many kinds of tasks, and it is constantly learning and improving.\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalChain.invoke(input=\"What is Claude?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ddbb73b-fc6b-4e1e-ab50-313e0fe55d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As Prof. Langchain told me, `ChatPromptTemplate` is a powerful tool for building conversational prompts within LangChain. It allows you to create structured prompts that are tailored for chat-like interactions, making it easier to guide your language models towards engaging and informative responses. \\n\\nHere's a breakdown of its key features:\\n\\n* **Chat-centric design:**  `ChatPromptTemplate` is specifically designed for chat-based interactions. It uses a conversational format that's more natural for users.\\n* **Structured prompts:** It allows you to define clear prompt structures, including placeholders for user input, context, and other variables. This ensures consistency and clarity in your prompts.\\n* **Flexible formatting:** You can easily customize the prompt's format, such as using different question formats, adding context, or incorporating specific instructions.\\n* **Integration with LLM chains:** `ChatPromptTemplate` seamlessly integrates with LangChain's LLM chains, making it easy to use your prompts with various language models.\\n\\nIn essence, `ChatPromptTemplate` simplifies the process of crafting conversational prompts, enabling you to build more engaging and effective chat applications with LangChain. \\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalChain.invoke(input=\"What is ChatPromptTemplate?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ab24136-caf7-44eb-8210-d3b64a714b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eklavya was a **legendary archer** in Hindu mythology, known for his unwavering dedication to archery and his tragic fate. Here's a breakdown of his story:\n",
      "\n",
      "* **Background:** Eklavya was a **tribal prince** from the Nishadha tribe, who was denied training by the renowned archer **Dronacharya** due to his caste. \n",
      "* **Self-Training:** Determined to learn archery, Eklavya created a clay statue of Dronacharya and practiced relentlessly, considering it his guru. \n",
      "* **Guru Dakshina:**  When Dronacharya discovered Eklavya's skill, he demanded his **thumb as a guru dakshina** (payment to a teacher). Eklavya readily agreed, sacrificing his thumb to fulfill his promise to his clay guru.\n",
      "* **Tragic Fate:**  While Eklavya's skill remained unparalleled, losing his thumb significantly hampered his archery. This act of sacrifice highlights the rigid caste system prevalent in ancient India and the consequences of prejudice.\n",
      "\n",
      "**Significance:**\n",
      "\n",
      "* **Symbol of Dedication:** Eklavya's story represents the power of dedication and self-belief in achieving mastery.\n",
      "* **Social Commentary:** It criticizes the caste system's inherent unfairness and the limitations imposed on individuals based on their birth.\n",
      "* **Moral Dilemma:** It raises ethical questions about the nature of mentorship, the consequences of prejudice, and the sacrifices one makes in pursuit of their passion.\n",
      "\n",
      "Eklavya's story continues to resonate with audiences today, offering valuable lessons about perseverance, social justice, and the complexities of human relationships. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(finalChain.invoke(input=\"Who was Eklavya?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b01777-d6b6-42c7-8270-ed0afd7488c4",
   "metadata": {},
   "source": [
    "<h3> Routing by Symantic Similarity</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce96bf6d-4460-405f-bea2-eed9ed03a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95cc795d-520f-440a-8785-aa357c078d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0cbd700-2293-4c9d-a6e9-21efc63dbaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI 1\\AppData\\Local\\Temp\\ipykernel_22416\\481481593.py:1: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings=SentenceTransformerEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "embeddings=SentenceTransformerEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6922a88-d874-4ad9-8571-9c97f8eed6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "promptTemplates=[physics_template,math_template]\n",
    "promptEmbeddings=embeddings.embed_documents(texts=promptTemplates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "843b526e-62d9-4fe0-a6d1-3ef35beb57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=[\"What's a Black Hole?\",\"What's Integral Calculus?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e87e6c2-01e1-41ff-b5ef-77b59aa5455e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25714636, 0.18505651],\n",
       "       [0.24262193, 0.27752088]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(X=embeddings.embed_documents(texts=questions),Y=promptEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "590d7d62-6faf-40a5-8192-1673eea4a5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(X=embeddings.embed_documents(texts=questions),Y=promptEmbeddings).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9be65c38-c7ef-47d9-9b6c-5ed729b38566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptRouter(input):\n",
    "    queryEmbedding=embeddings.embed_query(text=input['query'])\n",
    "    similarity=cosine_similarity(X=[queryEmbedding],Y=promptEmbeddings)[0]\n",
    "    mostSimilarTemplate=promptTemplates[similarity.argmax()]\n",
    "    print(\"Using Math Template\" if mostSimilarTemplate==math_template else \"Using Physics Template\")\n",
    "    return PromptTemplate.from_template(template=mostSimilarTemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a91dc0bd-31f9-4c51-879b-f41c5a244058",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain={\"query\":RunnablePassthrough()}|RunnableLambda(func=promptRouter)|llmGemini|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "008b9e43-8beb-4387-85cb-8373d9f468b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Physics Template\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A black hole is a region of spacetime where gravity is so strong that nothing, not even light, can escape. It's formed when a massive star collapses at the end of its life. Imagine squeezing the entire mass of our sun into a sphere the size of a city! \\n\\nHere's the key: the intense gravity of a black hole warps spacetime so much that it creates a point of no return called the event horizon.  Anything that crosses this boundary is forever trapped. \\n\\nWhile we can't directly see black holes, we can observe their effects on nearby matter and light. They act like cosmic vacuum cleaners, pulling in gas and dust, which heats up and emits radiation that we can detect. \\n\\nIt's a fascinating and still somewhat mysterious area of physics! \\n\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input=\"What's a Blackhole?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7599cf10-e283-408f-b84a-fa37ca5b5357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Math Template\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You\\'re right, breaking down problems is key to solving them, especially in math!  Let\\'s tackle \"What is trigonometry?\" using that approach:\\n\\n**1. The Building Blocks:**\\n\\n* **Triangles:** Trigonometry is all about the relationships between the sides and angles of triangles, particularly right triangles.\\n* **Angles:**  Angles are measured in degrees or radians.\\n* **Ratios:**  Trigonometry uses special ratios (sine, cosine, tangent) to describe the relationships between sides and angles.\\n\\n**2. The Ratios:**\\n\\n* **Sine (sin):**  The ratio of the side opposite an angle to the hypotenuse (the longest side).\\n* **Cosine (cos):** The ratio of the side adjacent to an angle to the hypotenuse.\\n* **Tangent (tan):** The ratio of the side opposite an angle to the side adjacent to the angle.\\n\\n**3. Putting it Together:**\\n\\nTrigonometry is a powerful tool for:\\n\\n* **Solving triangles:** Finding missing sides or angles in triangles.\\n* **Modeling periodic phenomena:**  Like waves, oscillations, and circular motion.\\n* **Understanding geometry and calculus:**  It forms the foundation for many advanced mathematical concepts.\\n\\n**In a nutshell:** Trigonometry is the study of triangles, their angles, and the relationships between their sides. It uses ratios to describe these relationships, which allows us to solve problems involving triangles and understand many real-world phenomena. \\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input=\"What's Trigonometry?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "461dc4d7-c8dc-4c25-bd63-d5410ac40d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " +----------------------+  \n",
      " | Parallel<query>Input |  \n",
      " +----------------------+  \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "      +-------------+      \n",
      "      | Passthrough |      \n",
      "      +-------------+      \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "     +--------------+      \n",
      "     | promptRouter |      \n",
      "     +--------------+      \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+------------------------+ \n",
      "| ChatGoogleGenerativeAI | \n",
      "+------------------------+ \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +-----------------+    \n",
      "    | StrOutputParser |    \n",
      "    +-----------------+    \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c12e04-0e6a-4ac7-ae03-d904f308683a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
